{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import json\n",
    "import requests \n",
    "from datetime import datetime\n",
    "\n",
    "\n",
    "def save_request(request_data, page_number, file_path='events_data.json'):\n",
    "# def save_request(request_data, page_number, file_path='events_data_231101-241101'):\n",
    "    # Load existing requests if the file exists\n",
    "    \n",
    "    if os.path.exists('datasets/'+ file_path):\n",
    "        with open('datasets/'+ file_path, 'r') as file:\n",
    "            requests = json.load(file)\n",
    "    else:\n",
    "        requests = []\n",
    "    \n",
    "    # Add timestamp to each event \n",
    "    new_request = []\n",
    "    tmp_timestamp = datetime.now().isoformat()\n",
    "\n",
    "\n",
    "    # add a timestamp to every object being processed\n",
    "    for tm_object in request_data:\n",
    "\n",
    "        tm_object['db_stamp'] = tmp_timestamp\n",
    "        requests.append(tm_object)\n",
    "    \n",
    "    \n",
    "    # Save updated requests back to the file\n",
    "    \n",
    "    with open('datasets/' + file_path, 'w') as file:\n",
    "        json.dump(requests, file, indent=4)\n",
    "\n",
    "    print(page_number)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pipeline to download information from an endpoint\n",
    "\n",
    "error_case = None\n",
    "def ticketmaster_download_data(object_to_retrieve,start= '2023-11-01T00:00:00Z',end ='2024-11-01T00:00:00Z',page_size = '80'):\n",
    "    \n",
    "    global error_case\n",
    "\n",
    "    print(f'Object to extract: {object_to_retrieve}')\n",
    "    #default values\n",
    "    i = 0               # to start in the first page\n",
    "    next_page = ''      # default page value\n",
    "\n",
    "    consumer_key = open('api_key.txt','r').read()\n",
    "    country_code = 'US'\n",
    "\n",
    "\n",
    "    base_url = 'https://app.ticketmaster.com'\n",
    "\n",
    "    url0 = f'https://app.ticketmaster.com/discovery/v2/{object_to_retrieve}.json?'\n",
    "    url0 += 'countryCode=' + country_code\n",
    "    url0 += '&startDateTime=' + start \n",
    "    url0 += '&endDateTime=' + end\n",
    "    # url0 += '&classificationName=' + 'music'\n",
    "    url0 += '&size=' + page_size + '&apikey=' + consumer_key\n",
    "\n",
    "    total_pages = 1\n",
    "    total_elements = 0\n",
    "\n",
    "    while i < total_pages:\n",
    "\n",
    "        # check if this is the first page to prepare\n",
    "        if i == 0:\n",
    "            \n",
    "            # get the first batch of information and retrieve the amount of pages to process\n",
    "            events_list = requests.request('GET', url0 )\n",
    "            total_pages = events_list.json()['page']['totalPages']\n",
    "            total_elements = events_list.json()['page']['totalElements']\n",
    "\n",
    "            # save the data requested\n",
    "            if total_pages==0:\n",
    "                print('There are no elements to retrieve.')\n",
    "                break\n",
    "            save_request(\n",
    "                events_list.json()['_embedded'][object_to_retrieve]\n",
    "                , str(events_list.json()['page']['number'])\n",
    "                , f'{object_to_retrieve}_data.json'\n",
    "                )\n",
    "            \n",
    "            # increase the page\n",
    "            i += 1\n",
    "            print(f'Total pages: {total_pages}')\n",
    "            if total_pages > 1000:\n",
    "                # break\n",
    "                print('it will break')\n",
    "\n",
    "        else:\n",
    "            # proceed in case there is a next page in the request data\n",
    "            if events_list.json().get('_links',{}).get('next','') != '':\n",
    "                # request the 'next' page in the link in case there are more data\n",
    "                events_list = requests.request('GET', base_url + events_list.json()['_links']['next']['href']+ '&apikey=' + consumer_key)\n",
    "                try:\n",
    "                    error_case = events_list\n",
    "                    save_request(\n",
    "                        events_list.json()['_embedded'][object_to_retrieve]\n",
    "                        , str(events_list.json()['page']['number'])\n",
    "                        , f'{object_to_retrieve}_data.json'\n",
    "                        )\n",
    "                    i += 1 \n",
    "                except Exception as e:\n",
    "                    print(e)\n",
    "            # stop downloading more \n",
    "            else:\n",
    "                break\n",
    "        \n",
    "        # in order to prevent the api_key to be throttled\n",
    "        time.sleep(1)\n",
    "\n",
    "    # unit test for validating the downloaded data\n",
    "\n",
    "    # load recently created json\n",
    "    with open(f'datasets/{object_to_retrieve}_data.json', 'r') as file:\n",
    "        full_data = json.load(file)\n",
    "\n",
    "    if len(full_data) == total_elements:\n",
    "        print(f'The download of the object {object_to_retrieve} was successful.')\n",
    "        print(f'Total elements downloaded: {total_elements}')\n",
    "    else:\n",
    "        print('There was an issue in the pipeline')\n",
    "        print('Here is the last request''s response ')\n",
    "        print('VVVVVVVV')\n",
    "        print(error_case.text)\n",
    "        print('')\n",
    "        print(f'Rows extracted: {len(full_data)}' )\n",
    "\n",
    "    # the json file needs to be formatted in the proper formatting for GCP\n",
    "    print('Prepare data to have BigQuery necessary formatting.')\n",
    "    with open(f'datasets/{object_to_retrieve}_data_f.json', \"w\") as new_file:\n",
    "        for row in full_data:\n",
    "            new_file.write(json.dumps(row))\n",
    "            new_file.write(\"\\n\")\n",
    "\n",
    "    # delete unformatted version of the data\n",
    "    os.remove(f'datasets/{object_to_retrieve}_data.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to upload raw data into BigQuery\n",
    "\n",
    "from google.cloud import bigquery\n",
    "\n",
    "# settup global variables for service-account connexion \n",
    "\n",
    "os.environ['GOOGLE_APPLICATION_CREDENTIALS'] = 'dbt_service_account_key.json'\n",
    "client = bigquery.Client(project='ticketmasterargodemo')\n",
    "\n",
    "\n",
    "def upload_data_to_bigquer(object_of_interest):\n",
    "    global client\n",
    "\n",
    "    filename = f'datasets/{object_of_interest}_data_f.json'\n",
    "    dataset_id = 'stage'\n",
    "    table_id = f'{object_of_interest}_tb'\n",
    "\n",
    "    dataset_ref = client.dataset(dataset_id)\n",
    "    table_ref = dataset_ref.table(table_id)\n",
    "    job_config = bigquery.LoadJobConfig()\n",
    "    job_config.source_format = bigquery.SourceFormat.NEWLINE_DELIMITED_JSON\n",
    "    job_config.autodetect = True\n",
    "\n",
    "    with open(filename, \"rb\") as source_file:\n",
    "        job = client.load_table_from_file(\n",
    "            source_file,\n",
    "            table_ref,\n",
    "            # location=\"us-east4\",  # This is region specific.\n",
    "            location=\"us\",  # This is a multiregion.\n",
    "            job_config=job_config,\n",
    "        )  # API request\n",
    "\n",
    "    job.result()  # Waits for table load to complete.\n",
    "\n",
    "    print(\"Loaded {} rows for object {} into {}:{}.\".format(job.output_rows, object_of_interest, dataset_id, table_id))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Object to extract: events\n",
      "0\n",
      "Total pages: 24\n",
      "1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception ignored in: <bound method IPythonKernel._clean_thread_parent_frames of <ipykernel.ipkernel.IPythonKernel object at 0x11ba5c6d0>>\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/anaconda3/envs/ticketmaster_poc/lib/python3.10/site-packages/ipykernel/ipkernel.py\", line 775, in _clean_thread_parent_frames\n",
      "    def _clean_thread_parent_frames(\n",
      "KeyboardInterrupt: \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n"
     ]
    }
   ],
   "source": [
    "start_of_month = datetime.utcnow().replace(day=1).strftime('%Y-%m-%dT00:00:00Z')\n",
    "current_date = datetime.utcnow().strftime('%Y-%m-%d') + 'T00:00:00Z'\n",
    "\n",
    "ticketmaster_download_data('events','2023-11-01T00:00:00Z',current_date,'50')\n",
    "print('')\n",
    "ticketmaster_download_data('attractions',start_of_month,current_date,'80') # this is pending\n",
    "print('')\n",
    "ticketmaster_download_data('venues',start_of_month,current_date,'80') # got limited to only 1000 records per deep-page request"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The table events has been cleaned.\n",
      "The table attractions has been cleaned.\n",
      "The table venues has been cleaned.\n"
     ]
    }
   ],
   "source": [
    "for object in ['events','attractions','venues']:\n",
    "    \n",
    "    # 3 , for deleting the data\n",
    "    query_string = f\"\"\"DROP TABLE `ticketmasterargodemo.stage.{object}_tb`;\"\"\"\n",
    "    results = client.query_and_wait(query_string)\n",
    "\n",
    "    print(f'The table {object} has been cleaned.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 820 rows for object events into stage:events_tb.\n",
      "Loaded 1040 rows for object attractions into stage:attractions_tb.\n",
      "Loaded 1040 rows for object venues into stage:venues_tb.\n"
     ]
    }
   ],
   "source": [
    "upload_data_to_bigquer('events')\n",
    "upload_data_to_bigquer('attractions')\n",
    "upload_data_to_bigquer('venues')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[0m04:58:09  Running with dbt=1.8.8\n",
      "\u001b[0m04:58:10  Registered adapter: bigquery=1.8.3\n",
      "\u001b[0m04:58:10  [\u001b[33mWARNING\u001b[0m]: Configuration paths exist in your dbt_project.yml file which do not apply to any resources.\n",
      "There are 2 unused configuration paths:\n",
      "- models.stage.stage_db\n",
      "- models.stage.production\n",
      "\u001b[0m04:58:10  Found 8 models, 4 data tests, 11 sources, 479 macros\n",
      "\u001b[0m04:58:10  \n",
      "\u001b[0m04:58:11  Concurrency: 4 threads (target='dev')\n",
      "\u001b[0m04:58:11  \n",
      "\u001b[0m04:58:11  1 of 6 START sql incremental model production.classification_elt ............... [RUN]\n",
      "\u001b[0m04:58:11  2 of 6 START sql incremental model production.event_attractions_elt ............ [RUN]\n",
      "\u001b[0m04:58:11  3 of 6 START sql incremental model production.events_elt ....................... [RUN]\n",
      "\u001b[0m04:58:11  4 of 6 START sql incremental model production.priceranges_elt .................. [RUN]\n",
      "\u001b[0m04:58:14  3 of 6 OK created sql incremental model production.events_elt .................. [\u001b[32mCREATE TABLE (820.0 rows, 231.4 KiB processed)\u001b[0m in 2.85s]\n",
      "\u001b[0m04:58:14  5 of 6 START sql incremental model production.products_elt ..................... [RUN]\n",
      "\u001b[0m04:58:14  1 of 6 OK created sql incremental model production.classification_elt .......... [\u001b[32mMERGE (0.0 rows, 88.8 KiB processed)\u001b[0m in 3.06s]\n",
      "\u001b[0m04:58:14  6 of 6 START sql incremental model production.venues_elt ....................... [RUN]\n",
      "\u001b[0m04:58:15  4 of 6 OK created sql incremental model production.priceranges_elt ............. [\u001b[32mMERGE (0.0 rows, 40.4 KiB processed)\u001b[0m in 3.35s]\n",
      "\u001b[0m04:58:15  2 of 6 OK created sql incremental model production.event_attractions_elt ....... [\u001b[32mMERGE (0.0 rows, 103.0 KiB processed)\u001b[0m in 3.40s]\n",
      "\u001b[0m04:58:18  5 of 6 OK created sql incremental model production.products_elt ................ [\u001b[32mMERGE (0.0 rows, 65.1 KiB processed)\u001b[0m in 3.43s]\n",
      "\u001b[0m04:58:18  6 of 6 OK created sql incremental model production.venues_elt .................. [\u001b[32mMERGE (0.0 rows, 646.6 KiB processed)\u001b[0m in 3.28s]\n",
      "\u001b[0m04:58:18  \n",
      "\u001b[0m04:58:18  Finished running 6 incremental models in 0 hours 0 minutes and 7.71 seconds (7.71s).\n",
      "\u001b[0m04:58:18  \n",
      "\u001b[0m04:58:18  \u001b[32mCompleted successfully\u001b[0m\n",
      "\u001b[0m04:58:18  \n",
      "\u001b[0m04:58:18  Done. PASS=6 WARN=0 ERROR=0 SKIP=0 TOTAL=6\n"
     ]
    }
   ],
   "source": [
    "!dbt run --select events_elt.sql classification_elt.sql event_attractions_elt.sql priceranges_elt.sql products_elt.sql venues_elt.sql"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TEST ENV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# queries to test connection\n",
    "# 1\n",
    "\n",
    "# query_string = \"\"\"SELECT name, SUM(number) as total\n",
    "# FROM `bigquery-public-data.usa_names.usa_1910_current`\n",
    "# WHERE name = 'William'\n",
    "# GROUP BY name;\n",
    "# \"\"\"\n",
    "# results = client.query_and_wait(query_string)\n",
    "\n",
    "# # Print the results.\n",
    "# for row in results:  # Wait for the job to complete.\n",
    "#     print(\"{}: {}\".format(row[\"name\"], row[\"total\"]))\n",
    "\n",
    "\n",
    "\n",
    "# 2\n",
    "# query_string = \"\"\"SELECT *\n",
    "# FROM `bigquer\n",
    "# y-public-data.usa_names.usa_1910_current`\n",
    "# WHERE name = 'William'\n",
    "# ;\n",
    "# \"\"\"\n",
    "# results = client.query_and_wait(query_string)\n",
    "\n",
    "# for row in results:\n",
    "#     print(row)\n",
    "\n",
    "\n",
    "# 3 , for deleting the data\n",
    "# query_string = \"\"\"TRUNCATE TABLE `ticketmaster-demo-argo.stage_db.events_tb`;\n",
    "# \"\"\"\n",
    "# results = client.query_and_wait(query_string)\n",
    "\n",
    "# for i in results:\n",
    "#     print(i)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ticketmaster_poc",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
