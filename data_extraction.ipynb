{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import json\n",
    "import requests \n",
    "from datetime import datetime\n",
    "\n",
    "\n",
    "def save_request(request_data, page_number, file_path='events_data.json'):\n",
    "# def save_request(request_data, page_number, file_path='events_data_231101-241101'):\n",
    "    # Load existing requests if the file exists\n",
    "    if os.path.exists('datasets/'+ file_path):\n",
    "        with open('datasets/'+ file_path, 'r') as file:\n",
    "            requests = json.load(file)\n",
    "    else:\n",
    "        requests = []\n",
    "    \n",
    "    # Add timestamp to each event \n",
    "    new_request = []\n",
    "    tmp_timestamp = datetime.now().isoformat()\n",
    "\n",
    "\n",
    "    # add a timestamp to every object being processed\n",
    "    for tm_object in request_data:\n",
    "\n",
    "        tm_object['db_stamp'] = tmp_timestamp\n",
    "        requests.append(tm_object)\n",
    "    \n",
    "    \n",
    "    # Save updated requests back to the file\n",
    "    \n",
    "    with open('datasets/' + file_path, 'w') as file:\n",
    "        json.dump(requests, file, indent=4)\n",
    "\n",
    "    print(page_number)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pipeline to download information from an endpoint\n",
    "\n",
    "error_case = None\n",
    "def ticketmaster_download_data(object_to_retrieve,start= '2023-11-01T00:00:00Z',end ='2024-11-01T00:00:00Z',page_size = '80'):\n",
    "    global error_case\n",
    "\n",
    "    print(f'Object to extract: {object_to_retrieve}')\n",
    "    #default values\n",
    "    i = 0               # to start in the first page\n",
    "    next_page = ''      # default page value\n",
    "\n",
    "    consumer_key = 'p5lYbtnQB3foTsOFRN7Jc7jpzeox9EMN'\n",
    "    country_code = 'US'\n",
    "\n",
    "\n",
    "    base_url = 'https://app.ticketmaster.com'\n",
    "\n",
    "    url0 = f'https://app.ticketmaster.com/discovery/v2/{object_to_retrieve}.json?'\n",
    "    url0 += 'countryCode=' + country_code\n",
    "    url0 += '&startDateTime=' + start \n",
    "    url0 += '&endDateTime=' + end\n",
    "    # url0 += '&classificationName=' + 'music'\n",
    "    url0 += '&size=' + page_size + '&apikey=' + consumer_key\n",
    "\n",
    "    total_pages = 1\n",
    "    total_elements = 0\n",
    "\n",
    "    while i < total_pages:\n",
    "\n",
    "        # check if this is the first page to prepare\n",
    "        if i == 0:\n",
    "            \n",
    "            # get the first batch of information and retrieve the amount of pages to process\n",
    "            events_list = requests.request('GET', url0 )\n",
    "            total_pages = events_list.json()['page']['totalPages']\n",
    "            total_elements = events_list.json()['page']['totalElements']\n",
    "\n",
    "            # save the data requested\n",
    "            if total_pages==0:\n",
    "                print('There are no elements to retrieve.')\n",
    "                break\n",
    "            save_request(\n",
    "                events_list.json()['_embedded'][object_to_retrieve]\n",
    "                , str(events_list.json()['page']['number'])\n",
    "                , f'{object_to_retrieve}_data.json'\n",
    "                )\n",
    "            \n",
    "            # increase the page\n",
    "            i += 1\n",
    "            print(f'Total pages: {total_pages}')\n",
    "            if total_pages > 1000:\n",
    "                # break\n",
    "                print('it will break')\n",
    "\n",
    "        else:\n",
    "            # proceed in case there is a next page in the request data\n",
    "            if events_list.json().get('_links',{}).get('next','') != '':\n",
    "                # request the 'next' page in the link in case there are more data\n",
    "                events_list = requests.request('GET', base_url + events_list.json()['_links']['next']['href']+ '&apikey=' + consumer_key)\n",
    "                try:\n",
    "                    error_case = events_list\n",
    "                    save_request(\n",
    "                        events_list.json()['_embedded'][object_to_retrieve]\n",
    "                        , str(events_list.json()['page']['number'])\n",
    "                        , f'{object_to_retrieve}_data.json'\n",
    "                        )\n",
    "                    i += 1 \n",
    "                except Exception as e:\n",
    "                    print(e)\n",
    "            # stop downloading more \n",
    "            else:\n",
    "                break\n",
    "        \n",
    "        # in order to prevent the api_key to be throttled\n",
    "        time.sleep(1)\n",
    "\n",
    "    # unit test for validating the downloaded data\n",
    "\n",
    "    # load recently created json\n",
    "    with open(f'datasets/{object_to_retrieve}_data.json', 'r') as file:\n",
    "        full_data = json.load(file)\n",
    "\n",
    "    if len(full_data) == total_elements:\n",
    "        print(f'The download of the object {object_to_retrieve} was successful.')\n",
    "        print(f'Total elements downloaded: {total_elements}')\n",
    "    else:\n",
    "        print('There was an issue in the pipeline')\n",
    "        print('Here is the last request''s response ')\n",
    "        print('VVVVVVVV')\n",
    "        print(error_case.text)\n",
    "        print('')\n",
    "        print(f'Rows extracted: {len(full_data)}' )\n",
    "\n",
    "    # the json file needs to be formatted in the proper formatting for GCP\n",
    "    print('Prepare data to have BigQuery necessary formatting.')\n",
    "    with open(f'datasets/{object_to_retrieve}_data_f.json', \"w\") as new_file:\n",
    "        for row in full_data:\n",
    "            new_file.write(json.dumps(row))\n",
    "            new_file.write(\"\\n\")\n",
    "\n",
    "    # delete unformatted version of the data\n",
    "    os.remove(f'datasets/{object_to_retrieve}_data.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to upload raw data into BigQuery\n",
    "\n",
    "from google.cloud import bigquery\n",
    "\n",
    "# settup global variables for service-account connexion \n",
    "\n",
    "os.environ['GOOGLE_APPLICATION_CREDENTIALS'] = 'service_key.json'\n",
    "client = bigquery.Client(project='ticketmaster-demo-argo')\n",
    "\n",
    "\n",
    "def upload_data_to_bigquer(object_of_interest):\n",
    "    global client\n",
    "\n",
    "    filename = f'datasets/{object_of_interest}_data_f.json'\n",
    "    dataset_id = 'stage_db'\n",
    "    table_id = f'{object_of_interest}_tb'\n",
    "\n",
    "    dataset_ref = client.dataset(dataset_id)\n",
    "    table_ref = dataset_ref.table(table_id)\n",
    "    job_config = bigquery.LoadJobConfig()\n",
    "    job_config.source_format = bigquery.SourceFormat.NEWLINE_DELIMITED_JSON\n",
    "    job_config.autodetect = True\n",
    "\n",
    "    with open(filename, \"rb\") as source_file:\n",
    "        job = client.load_table_from_file(\n",
    "            source_file,\n",
    "            table_ref,\n",
    "            location=\"us-east4\",  # Must match the destination dataset location.\n",
    "            job_config=job_config,\n",
    "        )  # API request\n",
    "\n",
    "    job.result()  # Waits for table load to complete.\n",
    "\n",
    "    print(\"Loaded {} rows for object {} into {}:{}.\".format(job.output_rows, object_of_interest, dataset_id, table_id))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Object to extract: events\n",
      "0\n",
      "Total pages: 10\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "The download of the object events was successful.\n",
      "Total elements downloaded: 494\n",
      "Prepare data to have BigQuery necessary formatting.\n",
      "\n",
      "Object to extract: attractions\n",
      "0\n",
      "Total pages: 4118\n",
      "it will break\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "11\n",
      "12\n",
      "'_embedded'\n",
      "There was an issue in the pipeline\n",
      "Here is the last requests response \n",
      "VVVVVVVV\n",
      "{\"errors\":[{\"code\":\"DIS1035\",\"detail\":\"API Limits Exceeded: Max paging depth exceeded. (page * size) must be less than 1,000\",\"status\":\"400\",\"_links\":{\"about\":{\"href\":\"/discovery/v2/errors.html#DIS1035\"}}}]}\n",
      "\n",
      "Rows extracted: 1040\n",
      "Prepare data to have BigQuery necessary formatting.\n",
      "\n",
      "Object to extract: venues\n",
      "0\n",
      "Total pages: 433\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "11\n",
      "12\n",
      "'_embedded'\n",
      "There was an issue in the pipeline\n",
      "Here is the last requests response \n",
      "VVVVVVVV\n",
      "{\"errors\":[{\"code\":\"DIS1035\",\"detail\":\"API Limits Exceeded: Max paging depth exceeded. (page * size) must be less than 1,000\",\"status\":\"400\",\"_links\":{\"about\":{\"href\":\"/discovery/v2/errors.html#DIS1035\"}}}]}\n",
      "\n",
      "Rows extracted: 1040\n",
      "Prepare data to have BigQuery necessary formatting.\n"
     ]
    }
   ],
   "source": [
    "ticketmaster_download_data('events','2023-11-01T00:00:00Z','2024-11-01T00:00:00Z','50')\n",
    "print('')\n",
    "ticketmaster_download_data('attractions','2024-10-15T00:00:00Z','2024-11-01T00:00:00Z','80') # this is pending\n",
    "print('')\n",
    "ticketmaster_download_data('venues','2024-10-01T00:00:00Z','2024-11-01T00:00:00Z','80') # got limited to only 1000 records per deep-page request"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The table events has been cleaned.\n",
      "The table attractions has been cleaned.\n",
      "The table venues has been cleaned.\n"
     ]
    }
   ],
   "source": [
    "for object in ['events','attractions','venues']:\n",
    "\n",
    "    # 3 , for deleting the data\n",
    "    query_string = f\"\"\"TRUNCATE TABLE `ticketmaster-demo-argo.stage_db.{object}_tb`;\"\"\"\n",
    "    results = client.query_and_wait(query_string)\n",
    "\n",
    "    print(f'The table {object} has been cleaned.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 494 rows for object events into stage_db:events_tb.\n",
      "Loaded 1040 rows for object attractions into stage_db:attractions_tb.\n",
      "Loaded 1040 rows for object venues into stage_db:venues_tb.\n"
     ]
    }
   ],
   "source": [
    "upload_data_to_bigquer('events')\n",
    "upload_data_to_bigquer('attractions')\n",
    "upload_data_to_bigquer('venues')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TEST ENV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# queries to test connection\n",
    "# 1\n",
    "\n",
    "# query_string = \"\"\"SELECT name, SUM(number) as total\n",
    "# FROM `bigquery-public-data.usa_names.usa_1910_current`\n",
    "# WHERE name = 'William'\n",
    "# GROUP BY name;\n",
    "# \"\"\"\n",
    "# results = client.query_and_wait(query_string)\n",
    "\n",
    "# # Print the results.\n",
    "# for row in results:  # Wait for the job to complete.\n",
    "#     print(\"{}: {}\".format(row[\"name\"], row[\"total\"]))\n",
    "\n",
    "\n",
    "\n",
    "# 2\n",
    "# query_string = \"\"\"SELECT *\n",
    "# FROM `bigquery-public-data.usa_names.usa_1910_current`\n",
    "# WHERE name = 'William'\n",
    "# ;\n",
    "# \"\"\"\n",
    "# results = client.query_and_wait(query_string)\n",
    "\n",
    "# for row in results:\n",
    "#     print(row)\n",
    "\n",
    "\n",
    "# 3 , for deleting the data\n",
    "# query_string = \"\"\"TRUNCATE TABLE `ticketmaster-demo-argo.stage_db.events_tb`;\n",
    "# \"\"\"\n",
    "# results = client.query_and_wait(query_string)\n",
    "\n",
    "# for i in results:\n",
    "#     print(i)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ticketmaster_poc",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
