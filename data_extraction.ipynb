{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import json\n",
    "import requests \n",
    "from datetime import datetime\n",
    "\n",
    "\n",
    "def save_request(request_data, page_number, file_path='events_data.json'):\n",
    "# def save_request(request_data, page_number, file_path='events_data_231101-241101'):\n",
    "    # Load existing requests if the file exists\n",
    "    \n",
    "    if os.path.exists('datasets/'+ file_path):\n",
    "        with open('datasets/'+ file_path, 'r') as file:\n",
    "            requests = json.load(file)\n",
    "    else:\n",
    "        requests = []\n",
    "    \n",
    "    # Add timestamp to each event \n",
    "    new_request = []\n",
    "    tmp_timestamp = datetime.now().isoformat()\n",
    "\n",
    "\n",
    "    # add a timestamp to every object being processed\n",
    "    for tm_object in request_data:\n",
    "\n",
    "        tm_object['db_stamp'] = tmp_timestamp\n",
    "        requests.append(tm_object)\n",
    "    \n",
    "    \n",
    "    # Save updated requests back to the file\n",
    "    \n",
    "    with open('datasets/' + file_path, 'w') as file:\n",
    "        json.dump(requests, file, indent=4)\n",
    "\n",
    "    print(page_number)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pipeline to download information from an endpoint\n",
    "\n",
    "error_case = None\n",
    "def ticketmaster_download_data(object_to_retrieve,endstart='',start= '2023-11-01T00:00:00Z',end ='2024-11-01T00:00:00Z',page_size = '80'):\n",
    "    \n",
    "    global error_case\n",
    "\n",
    "    print(f'Object to extract: {object_to_retrieve}')\n",
    "    #default values\n",
    "    i = 0               # to start in the first page\n",
    "    next_page = ''      # default page value\n",
    "\n",
    "    consumer_key = open('api_key.txt','r').read()\n",
    "    country_code = 'US'\n",
    "\n",
    "\n",
    "    base_url = 'https://app.ticketmaster.com'\n",
    "\n",
    "    url0 = f'https://app.ticketmaster.com/discovery/v2/{object_to_retrieve}.json?'\n",
    "    url0 += 'countryCode=' + country_code\n",
    "    if endstart != '':\n",
    "        url0 += '&startEndDateTime='+endstart\n",
    "    if start != '':\n",
    "        # url0 += '&startDateTime=' + start \n",
    "        url0 += '&onsaleStartDateTime=' + start \n",
    "    if end != '':\n",
    "        # url0 += '&endDateTime=' + end\n",
    "        url0 += '&onsaleEndDateTime=' + end\n",
    "    # url0 += '&classificationName=' + 'music'\n",
    "    url0 += '&size=' + page_size + '&apikey=' + consumer_key\n",
    "\n",
    "    total_pages = 1\n",
    "    total_elements = 0\n",
    "\n",
    "    while i < total_pages:\n",
    "\n",
    "        # check if this is the first page to prepare\n",
    "        if i == 0:\n",
    "            \n",
    "            # get the first batch of information and retrieve the amount of pages to process\n",
    "            events_list = requests.request('GET', url0 )\n",
    "            total_pages = events_list.json()['page']['totalPages']\n",
    "            total_elements = events_list.json()['page']['totalElements']\n",
    "\n",
    "            # save the data requested\n",
    "            if total_pages==0:\n",
    "                print('There are no elements to retrieve.')\n",
    "                break\n",
    "            save_request(\n",
    "                events_list.json()['_embedded'][object_to_retrieve]\n",
    "                , str(events_list.json()['page']['number'])\n",
    "                , f'{object_to_retrieve}_data.json'\n",
    "                )\n",
    "            \n",
    "            # increase the page\n",
    "            i += 1\n",
    "            print(f'Total pages: {total_pages}')\n",
    "            print(f'Total enries: {total_elements}')\n",
    "            if total_pages > 1000:\n",
    "                # break\n",
    "                print('it will break')\n",
    "\n",
    "        else:\n",
    "            # proceed in case there is a next page in the request data\n",
    "            if events_list.json().get('_links',{}).get('next','') != '':\n",
    "                # request the 'next' page in the link in case there are more data\n",
    "                events_list = requests.request('GET', base_url + events_list.json()['_links']['next']['href']+ '&apikey=' + consumer_key)\n",
    "                try:\n",
    "                    error_case = events_list\n",
    "                    save_request(\n",
    "                        events_list.json()['_embedded'][object_to_retrieve]\n",
    "                        , str(events_list.json()['page']['number'])\n",
    "                        , f'{object_to_retrieve}_data.json'\n",
    "                        )\n",
    "                    i += 1 \n",
    "                except Exception as e:\n",
    "                    print(e)\n",
    "            # stop downloading more \n",
    "            else:\n",
    "                break\n",
    "        \n",
    "        # in order to prevent the api_key to be throttled\n",
    "        time.sleep(1)\n",
    "\n",
    "    # unit test for validating the downloaded data\n",
    "\n",
    "    # load recently created json\n",
    "    with open(f'datasets/{object_to_retrieve}_data.json', 'r') as file:\n",
    "        full_data = json.load(file)\n",
    "\n",
    "    if len(full_data) == total_elements:\n",
    "        print(f'The download of the object {object_to_retrieve} was successful.')\n",
    "        print(f'Total elements downloaded: {total_elements}')\n",
    "    else:\n",
    "        print('There was an issue in the pipeline')\n",
    "        print('Here is the last request''s response ')\n",
    "        print('VVVVVVVV')\n",
    "        print(error_case.text)\n",
    "        print('')\n",
    "        print(f'Rows extracted: {len(full_data)}' )\n",
    "\n",
    "    # the json file needs to be formatted in the proper formatting for GCP\n",
    "    print('Prepare data to have BigQuery necessary formatting.')\n",
    "    with open(f'datasets/{object_to_retrieve}_data_f.json', \"w\") as new_file:\n",
    "        for row in full_data:\n",
    "            new_file.write(json.dumps(row))\n",
    "            new_file.write(\"\\n\")\n",
    "\n",
    "    # delete unformatted version of the data\n",
    "    os.remove(f'datasets/{object_to_retrieve}_data.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to upload raw data into BigQuery\n",
    "\n",
    "from google.cloud import bigquery\n",
    "\n",
    "# settup global variables for service-account connexion \n",
    "\n",
    "os.environ['GOOGLE_APPLICATION_CREDENTIALS'] = 'dbt_service_account_key.json'\n",
    "client = bigquery.Client(project='ticketmasterargodemo')\n",
    "\n",
    "\n",
    "def upload_data_to_bigquer(object_of_interest):\n",
    "    global client\n",
    "\n",
    "    filename = f'datasets/{object_of_interest}_data_f.json'\n",
    "    dataset_id = 'stage'\n",
    "    table_id = f'{object_of_interest}_tb'\n",
    "\n",
    "    dataset_ref = client.dataset(dataset_id)\n",
    "    table_ref = dataset_ref.table(table_id)\n",
    "    job_config = bigquery.LoadJobConfig()\n",
    "    job_config.source_format = bigquery.SourceFormat.NEWLINE_DELIMITED_JSON\n",
    "    job_config.autodetect = True\n",
    "\n",
    "    with open(filename, \"rb\") as source_file:\n",
    "        job = client.load_table_from_file(\n",
    "            source_file,\n",
    "            table_ref,\n",
    "            # location=\"us-east4\",  # This is region specific.\n",
    "            location=\"us\",  # This is a multiregion.\n",
    "            job_config=job_config,\n",
    "        )  # API request\n",
    "\n",
    "    job.result()  # Waits for table load to complete.\n",
    "\n",
    "    print(\"Loaded {} rows for object {} into {}:{}.\".format(job.output_rows, object_of_interest, dataset_id, table_id))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Object to extract: events\n",
      "0\n",
      "Total pages: 663\n",
      "Total enries: 33116\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "11\n",
      "12\n",
      "13\n",
      "14\n",
      "15\n",
      "16\n",
      "17\n",
      "18\n",
      "19\n",
      "'_embedded'\n",
      "There was an issue in the pipeline\n",
      "Here is the last requests response \n",
      "VVVVVVVV\n",
      "{\"errors\":[{\"code\":\"DIS1035\",\"detail\":\"API Limits Exceeded: Max paging depth exceeded. (page * size) must be less than 1,000\",\"status\":\"400\",\"_links\":{\"about\":{\"href\":\"/discovery/v2/errors.html#DIS1035\"}}}]}\n",
      "\n",
      "Rows extracted: 1000\n",
      "Prepare data to have BigQuery necessary formatting.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "start_of_month = datetime.utcnow().replace(day=1).strftime('%Y-%m-%dT00:00:00Z')\n",
    "current_date = datetime.utcnow().strftime('%Y-%m-%d') + 'T00:00:00Z'\n",
    "\n",
    "date_to_process = '2024-03-01T00:00:00Z'\n",
    "\n",
    "# ticketmaster_download_data('events','2024-11-05T00:00:00Z','2024-11-18T00:00:00Z','50')\n",
    "ticketmaster_download_data('events','',date_to_process,date_to_process,'50')\n",
    "print('')\n",
    "# ticketmaster_download_data('attractions',start_of_month,current_date,'80') # this is pending\n",
    "# print('')\n",
    "# ticketmaster_download_data('venues',start_of_month,current_date,'80') # got limited to only 1000 records per deep-page request"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "ename": "NotFound",
     "evalue": "404 POST https://bigquery.googleapis.com/bigquery/v2/projects/ticketmasterargodemo/queries?prettyPrint=false: Not found: Table ticketmasterargodemo:stage.events_tb",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNotFound\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[120], line 6\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m \u001b[38;5;28mobject\u001b[39m \u001b[38;5;129;01min\u001b[39;00m [\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mevents\u001b[39m\u001b[38;5;124m'\u001b[39m]:\n\u001b[1;32m      3\u001b[0m     \n\u001b[1;32m      4\u001b[0m     \u001b[38;5;66;03m# 3 , for deleting the data\u001b[39;00m\n\u001b[1;32m      5\u001b[0m     query_string \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\"\"\u001b[39m\u001b[38;5;124mDROP TABLE `ticketmasterargodemo.stage.\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mobject\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m_tb`;\u001b[39m\u001b[38;5;124m\"\"\"\u001b[39m\n\u001b[0;32m----> 6\u001b[0m     results \u001b[38;5;241m=\u001b[39m \u001b[43mclient\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mquery_and_wait\u001b[49m\u001b[43m(\u001b[49m\u001b[43mquery_string\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      8\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mThe table \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mobject\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m has been cleaned.\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "File \u001b[0;32m/opt/anaconda3/envs/ticketmaster_poc/lib/python3.10/site-packages/google/cloud/bigquery/client.py:3601\u001b[0m, in \u001b[0;36mClient.query_and_wait\u001b[0;34m(self, query, job_config, location, project, api_timeout, wait_timeout, retry, job_retry, page_size, max_results)\u001b[0m\n\u001b[1;32m   3595\u001b[0m     _verify_job_config_type(job_config, QueryJobConfig)\n\u001b[1;32m   3597\u001b[0m job_config \u001b[38;5;241m=\u001b[39m _job_helpers\u001b[38;5;241m.\u001b[39mjob_config_with_defaults(\n\u001b[1;32m   3598\u001b[0m     job_config, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_default_query_job_config\n\u001b[1;32m   3599\u001b[0m )\n\u001b[0;32m-> 3601\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_job_helpers\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mquery_and_wait\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   3602\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3603\u001b[0m \u001b[43m    \u001b[49m\u001b[43mquery\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3604\u001b[0m \u001b[43m    \u001b[49m\u001b[43mjob_config\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mjob_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3605\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlocation\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlocation\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3606\u001b[0m \u001b[43m    \u001b[49m\u001b[43mproject\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mproject\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3607\u001b[0m \u001b[43m    \u001b[49m\u001b[43mapi_timeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mapi_timeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3608\u001b[0m \u001b[43m    \u001b[49m\u001b[43mwait_timeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mwait_timeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3609\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretry\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mretry\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3610\u001b[0m \u001b[43m    \u001b[49m\u001b[43mjob_retry\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mjob_retry\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3611\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpage_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpage_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3612\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmax_results\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmax_results\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3613\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/anaconda3/envs/ticketmaster_poc/lib/python3.10/site-packages/google/cloud/bigquery/_job_helpers.py:509\u001b[0m, in \u001b[0;36mquery_and_wait\u001b[0;34m(client, query, job_config, location, project, api_timeout, wait_timeout, retry, job_retry, page_size, max_results)\u001b[0m\n\u001b[1;32m    492\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m table\u001b[38;5;241m.\u001b[39mRowIterator(\n\u001b[1;32m    493\u001b[0m         client\u001b[38;5;241m=\u001b[39mclient,\n\u001b[1;32m    494\u001b[0m         api_request\u001b[38;5;241m=\u001b[39mfunctools\u001b[38;5;241m.\u001b[39mpartial(client\u001b[38;5;241m.\u001b[39m_call_api, retry, timeout\u001b[38;5;241m=\u001b[39mapi_timeout),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    505\u001b[0m         num_dml_affected_rows\u001b[38;5;241m=\u001b[39mquery_results\u001b[38;5;241m.\u001b[39mnum_dml_affected_rows,\n\u001b[1;32m    506\u001b[0m     )\n\u001b[1;32m    508\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m job_retry \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mjob_retry\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdo_query\u001b[49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m do_query()\n",
      "File \u001b[0;32m/opt/anaconda3/envs/ticketmaster_poc/lib/python3.10/site-packages/google/api_core/retry/retry_unary.py:293\u001b[0m, in \u001b[0;36mRetry.__call__.<locals>.retry_wrapped_func\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    289\u001b[0m target \u001b[38;5;241m=\u001b[39m functools\u001b[38;5;241m.\u001b[39mpartial(func, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    290\u001b[0m sleep_generator \u001b[38;5;241m=\u001b[39m exponential_sleep_generator(\n\u001b[1;32m    291\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_initial, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_maximum, multiplier\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_multiplier\n\u001b[1;32m    292\u001b[0m )\n\u001b[0;32m--> 293\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mretry_target\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    294\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtarget\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    295\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_predicate\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    296\u001b[0m \u001b[43m    \u001b[49m\u001b[43msleep_generator\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    297\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_timeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    298\u001b[0m \u001b[43m    \u001b[49m\u001b[43mon_error\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mon_error\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    299\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/anaconda3/envs/ticketmaster_poc/lib/python3.10/site-packages/google/api_core/retry/retry_unary.py:153\u001b[0m, in \u001b[0;36mretry_target\u001b[0;34m(target, predicate, sleep_generator, timeout, on_error, exception_factory, **kwargs)\u001b[0m\n\u001b[1;32m    149\u001b[0m \u001b[38;5;66;03m# pylint: disable=broad-except\u001b[39;00m\n\u001b[1;32m    150\u001b[0m \u001b[38;5;66;03m# This function explicitly must deal with broad exceptions.\u001b[39;00m\n\u001b[1;32m    151\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m exc:\n\u001b[1;32m    152\u001b[0m     \u001b[38;5;66;03m# defer to shared logic for handling errors\u001b[39;00m\n\u001b[0;32m--> 153\u001b[0m     \u001b[43m_retry_error_helper\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    154\u001b[0m \u001b[43m        \u001b[49m\u001b[43mexc\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    155\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdeadline\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    156\u001b[0m \u001b[43m        \u001b[49m\u001b[43msleep\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    157\u001b[0m \u001b[43m        \u001b[49m\u001b[43merror_list\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    158\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpredicate\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    159\u001b[0m \u001b[43m        \u001b[49m\u001b[43mon_error\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    160\u001b[0m \u001b[43m        \u001b[49m\u001b[43mexception_factory\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    161\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    162\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    163\u001b[0m     \u001b[38;5;66;03m# if exception not raised, sleep before next attempt\u001b[39;00m\n\u001b[1;32m    164\u001b[0m     time\u001b[38;5;241m.\u001b[39msleep(sleep)\n",
      "File \u001b[0;32m/opt/anaconda3/envs/ticketmaster_poc/lib/python3.10/site-packages/google/api_core/retry/retry_base.py:212\u001b[0m, in \u001b[0;36m_retry_error_helper\u001b[0;34m(exc, deadline, next_sleep, error_list, predicate_fn, on_error_fn, exc_factory_fn, original_timeout)\u001b[0m\n\u001b[1;32m    206\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m predicate_fn(exc):\n\u001b[1;32m    207\u001b[0m     final_exc, source_exc \u001b[38;5;241m=\u001b[39m exc_factory_fn(\n\u001b[1;32m    208\u001b[0m         error_list,\n\u001b[1;32m    209\u001b[0m         RetryFailureReason\u001b[38;5;241m.\u001b[39mNON_RETRYABLE_ERROR,\n\u001b[1;32m    210\u001b[0m         original_timeout,\n\u001b[1;32m    211\u001b[0m     )\n\u001b[0;32m--> 212\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m final_exc \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msource_exc\u001b[39;00m\n\u001b[1;32m    213\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m on_error_fn \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    214\u001b[0m     on_error_fn(exc)\n",
      "File \u001b[0;32m/opt/anaconda3/envs/ticketmaster_poc/lib/python3.10/site-packages/google/api_core/retry/retry_unary.py:144\u001b[0m, in \u001b[0;36mretry_target\u001b[0;34m(target, predicate, sleep_generator, timeout, on_error, exception_factory, **kwargs)\u001b[0m\n\u001b[1;32m    142\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m sleep \u001b[38;5;129;01min\u001b[39;00m sleep_generator:\n\u001b[1;32m    143\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 144\u001b[0m         result \u001b[38;5;241m=\u001b[39m \u001b[43mtarget\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    145\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m inspect\u001b[38;5;241m.\u001b[39misawaitable(result):\n\u001b[1;32m    146\u001b[0m             warnings\u001b[38;5;241m.\u001b[39mwarn(_ASYNC_RETRY_WARNING)\n",
      "File \u001b[0;32m/opt/anaconda3/envs/ticketmaster_poc/lib/python3.10/site-packages/google/cloud/bigquery/_job_helpers.py:450\u001b[0m, in \u001b[0;36mquery_and_wait.<locals>.do_query\u001b[0;34m()\u001b[0m\n\u001b[1;32m    448\u001b[0m \u001b[38;5;66;03m# For easier testing, handle the retries ourselves.\u001b[39;00m\n\u001b[1;32m    449\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m retry \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 450\u001b[0m     response \u001b[38;5;241m=\u001b[39m \u001b[43mretry\u001b[49m\u001b[43m(\u001b[49m\u001b[43mclient\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_api\u001b[49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    451\u001b[0m \u001b[43m        \u001b[49m\u001b[43mretry\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# We're calling the retry decorator ourselves.\u001b[39;49;00m\n\u001b[1;32m    452\u001b[0m \u001b[43m        \u001b[49m\u001b[43mspan_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mBigQuery.query\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    453\u001b[0m \u001b[43m        \u001b[49m\u001b[43mspan_attributes\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mspan_attributes\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    454\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmethod\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mPOST\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    455\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpath\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpath\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    456\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdata\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrequest_body\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    457\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mapi_timeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    458\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    459\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    460\u001b[0m     response \u001b[38;5;241m=\u001b[39m client\u001b[38;5;241m.\u001b[39m_call_api(\n\u001b[1;32m    461\u001b[0m         retry\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m    462\u001b[0m         span_name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mBigQuery.query\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    467\u001b[0m         timeout\u001b[38;5;241m=\u001b[39mapi_timeout,\n\u001b[1;32m    468\u001b[0m     )\n",
      "File \u001b[0;32m/opt/anaconda3/envs/ticketmaster_poc/lib/python3.10/site-packages/google/api_core/retry/retry_unary.py:293\u001b[0m, in \u001b[0;36mRetry.__call__.<locals>.retry_wrapped_func\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    289\u001b[0m target \u001b[38;5;241m=\u001b[39m functools\u001b[38;5;241m.\u001b[39mpartial(func, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    290\u001b[0m sleep_generator \u001b[38;5;241m=\u001b[39m exponential_sleep_generator(\n\u001b[1;32m    291\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_initial, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_maximum, multiplier\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_multiplier\n\u001b[1;32m    292\u001b[0m )\n\u001b[0;32m--> 293\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mretry_target\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    294\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtarget\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    295\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_predicate\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    296\u001b[0m \u001b[43m    \u001b[49m\u001b[43msleep_generator\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    297\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_timeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    298\u001b[0m \u001b[43m    \u001b[49m\u001b[43mon_error\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mon_error\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    299\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/anaconda3/envs/ticketmaster_poc/lib/python3.10/site-packages/google/api_core/retry/retry_unary.py:153\u001b[0m, in \u001b[0;36mretry_target\u001b[0;34m(target, predicate, sleep_generator, timeout, on_error, exception_factory, **kwargs)\u001b[0m\n\u001b[1;32m    149\u001b[0m \u001b[38;5;66;03m# pylint: disable=broad-except\u001b[39;00m\n\u001b[1;32m    150\u001b[0m \u001b[38;5;66;03m# This function explicitly must deal with broad exceptions.\u001b[39;00m\n\u001b[1;32m    151\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m exc:\n\u001b[1;32m    152\u001b[0m     \u001b[38;5;66;03m# defer to shared logic for handling errors\u001b[39;00m\n\u001b[0;32m--> 153\u001b[0m     \u001b[43m_retry_error_helper\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    154\u001b[0m \u001b[43m        \u001b[49m\u001b[43mexc\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    155\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdeadline\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    156\u001b[0m \u001b[43m        \u001b[49m\u001b[43msleep\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    157\u001b[0m \u001b[43m        \u001b[49m\u001b[43merror_list\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    158\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpredicate\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    159\u001b[0m \u001b[43m        \u001b[49m\u001b[43mon_error\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    160\u001b[0m \u001b[43m        \u001b[49m\u001b[43mexception_factory\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    161\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    162\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    163\u001b[0m     \u001b[38;5;66;03m# if exception not raised, sleep before next attempt\u001b[39;00m\n\u001b[1;32m    164\u001b[0m     time\u001b[38;5;241m.\u001b[39msleep(sleep)\n",
      "File \u001b[0;32m/opt/anaconda3/envs/ticketmaster_poc/lib/python3.10/site-packages/google/api_core/retry/retry_base.py:212\u001b[0m, in \u001b[0;36m_retry_error_helper\u001b[0;34m(exc, deadline, next_sleep, error_list, predicate_fn, on_error_fn, exc_factory_fn, original_timeout)\u001b[0m\n\u001b[1;32m    206\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m predicate_fn(exc):\n\u001b[1;32m    207\u001b[0m     final_exc, source_exc \u001b[38;5;241m=\u001b[39m exc_factory_fn(\n\u001b[1;32m    208\u001b[0m         error_list,\n\u001b[1;32m    209\u001b[0m         RetryFailureReason\u001b[38;5;241m.\u001b[39mNON_RETRYABLE_ERROR,\n\u001b[1;32m    210\u001b[0m         original_timeout,\n\u001b[1;32m    211\u001b[0m     )\n\u001b[0;32m--> 212\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m final_exc \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msource_exc\u001b[39;00m\n\u001b[1;32m    213\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m on_error_fn \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    214\u001b[0m     on_error_fn(exc)\n",
      "File \u001b[0;32m/opt/anaconda3/envs/ticketmaster_poc/lib/python3.10/site-packages/google/api_core/retry/retry_unary.py:144\u001b[0m, in \u001b[0;36mretry_target\u001b[0;34m(target, predicate, sleep_generator, timeout, on_error, exception_factory, **kwargs)\u001b[0m\n\u001b[1;32m    142\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m sleep \u001b[38;5;129;01min\u001b[39;00m sleep_generator:\n\u001b[1;32m    143\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 144\u001b[0m         result \u001b[38;5;241m=\u001b[39m \u001b[43mtarget\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    145\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m inspect\u001b[38;5;241m.\u001b[39misawaitable(result):\n\u001b[1;32m    146\u001b[0m             warnings\u001b[38;5;241m.\u001b[39mwarn(_ASYNC_RETRY_WARNING)\n",
      "File \u001b[0;32m/opt/anaconda3/envs/ticketmaster_poc/lib/python3.10/site-packages/google/cloud/bigquery/client.py:833\u001b[0m, in \u001b[0;36mClient._call_api\u001b[0;34m(self, retry, span_name, span_attributes, job_ref, headers, **kwargs)\u001b[0m\n\u001b[1;32m    829\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m span_name \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    830\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m create_span(\n\u001b[1;32m    831\u001b[0m         name\u001b[38;5;241m=\u001b[39mspan_name, attributes\u001b[38;5;241m=\u001b[39mspan_attributes, client\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m, job_ref\u001b[38;5;241m=\u001b[39mjob_ref\n\u001b[1;32m    832\u001b[0m     ):\n\u001b[0;32m--> 833\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mcall\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    835\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m call()\n",
      "File \u001b[0;32m/opt/anaconda3/envs/ticketmaster_poc/lib/python3.10/site-packages/google/cloud/_http/__init__.py:494\u001b[0m, in \u001b[0;36mJSONConnection.api_request\u001b[0;34m(self, method, path, query_params, data, content_type, headers, api_base_url, api_version, expect_json, _target_object, timeout, extra_api_info)\u001b[0m\n\u001b[1;32m    482\u001b[0m response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_make_request(\n\u001b[1;32m    483\u001b[0m     method\u001b[38;5;241m=\u001b[39mmethod,\n\u001b[1;32m    484\u001b[0m     url\u001b[38;5;241m=\u001b[39murl,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    490\u001b[0m     extra_api_info\u001b[38;5;241m=\u001b[39mextra_api_info,\n\u001b[1;32m    491\u001b[0m )\n\u001b[1;32m    493\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;241m200\u001b[39m \u001b[38;5;241m<\u001b[39m\u001b[38;5;241m=\u001b[39m response\u001b[38;5;241m.\u001b[39mstatus_code \u001b[38;5;241m<\u001b[39m \u001b[38;5;241m300\u001b[39m:\n\u001b[0;32m--> 494\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m exceptions\u001b[38;5;241m.\u001b[39mfrom_http_response(response)\n\u001b[1;32m    496\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m expect_json \u001b[38;5;129;01mand\u001b[39;00m response\u001b[38;5;241m.\u001b[39mcontent:\n\u001b[1;32m    497\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m response\u001b[38;5;241m.\u001b[39mjson()\n",
      "\u001b[0;31mNotFound\u001b[0m: 404 POST https://bigquery.googleapis.com/bigquery/v2/projects/ticketmasterargodemo/queries?prettyPrint=false: Not found: Table ticketmasterargodemo:stage.events_tb"
     ]
    }
   ],
   "source": [
    "# for object in ['events','attractions','venues']:\n",
    "for object in ['events']:\n",
    "    \n",
    "    # 3 , for deleting the data\n",
    "    query_string = f\"\"\"DROP TABLE `ticketmasterargodemo.stage.{object}_tb`;\"\"\"\n",
    "    results = client.query_and_wait(query_string)\n",
    "\n",
    "    print(f'The table {object} has been cleaned.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 1000 rows for object events into stage:events_tb.\n"
     ]
    }
   ],
   "source": [
    "upload_data_to_bigquer('events')\n",
    "# upload_data_to_bigquer('attractions')\n",
    "# upload_data_to_bigquer('venues')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[0m05:10:01  Running with dbt=1.8.8\n",
      "\u001b[0m05:10:01  Registered adapter: bigquery=1.8.3\n",
      "\u001b[0m05:10:01  [\u001b[33mWARNING\u001b[0m]: Configuration paths exist in your dbt_project.yml file which do not apply to any resources.\n",
      "There are 2 unused configuration paths:\n",
      "- models.stage.production\n",
      "- models.stage.stage_db\n",
      "\u001b[0m05:10:01  Found 9 models, 4 data tests, 11 sources, 479 macros\n",
      "\u001b[0m05:10:01  \n",
      "\u001b[0m05:10:03  Concurrency: 4 threads (target='dev')\n",
      "\u001b[0m05:10:03  \n",
      "\u001b[0m05:10:03  1 of 6 START sql incremental model production.classification_elt ............... [RUN]\n",
      "\u001b[0m05:10:03  2 of 6 START sql incremental model production.event_attractions_elt ............ [RUN]\n",
      "\u001b[0m05:10:03  3 of 6 START sql incremental model production.events_elt ....................... [RUN]\n",
      "\u001b[0m05:10:03  4 of 6 START sql incremental model production.priceranges_elt .................. [RUN]\n",
      "\u001b[0m05:10:06  3 of 6 OK created sql incremental model production.events_elt .................. [\u001b[32mMERGE (0.0 rows, 801.2 KiB processed)\u001b[0m in 3.18s]\n",
      "\u001b[0m05:10:06  4 of 6 OK created sql incremental model production.priceranges_elt ............. [\u001b[32mMERGE (0.0 rows, 75.7 KiB processed)\u001b[0m in 3.18s]\n",
      "\u001b[0m05:10:06  5 of 6 START sql incremental model production.products_elt ..................... [RUN]\n",
      "\u001b[0m05:10:06  6 of 6 START sql incremental model production.venues_elt ....................... [RUN]\n",
      "\u001b[0m05:10:06  2 of 6 OK created sql incremental model production.event_attractions_elt ....... [\u001b[32mMERGE (0.0 rows, 395.1 KiB processed)\u001b[0m in 3.19s]\n",
      "\u001b[0m05:10:09  5 of 6 OK created sql incremental model production.products_elt ................ [\u001b[32mMERGE (0.0 rows, 410.4 KiB processed)\u001b[0m in 3.06s]\n",
      "\u001b[0m05:10:09  6 of 6 OK created sql incremental model production.venues_elt .................. [\u001b[32mMERGE (0.0 rows, 2.1 MiB processed)\u001b[0m in 3.41s]\n",
      "\u001b[0m05:10:10  1 of 6 OK created sql incremental model production.classification_elt .......... [\u001b[32mMERGE (0.0 rows, 200.5 KiB processed)\u001b[0m in 7.68s]\n",
      "\u001b[0m05:10:10  \n",
      "\u001b[0m05:10:10  Finished running 6 incremental models in 0 hours 0 minutes and 9.04 seconds (9.04s).\n",
      "\u001b[0m05:10:10  \n",
      "\u001b[0m05:10:10  \u001b[32mCompleted successfully\u001b[0m\n",
      "\u001b[0m05:10:10  \n",
      "\u001b[0m05:10:10  Done. PASS=6 WARN=0 ERROR=0 SKIP=0 TOTAL=6\n"
     ]
    }
   ],
   "source": [
    "!dbt run --select events_elt.sql classification_elt.sql event_attractions_elt.sql priceranges_elt.sql products_elt.sql venues_elt.sql"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TEST ENV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Object to extract: events\n",
      "0\n",
      "Total pages: 559\n",
      "Total enries: 27909\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "11\n",
      "12\n",
      "13\n",
      "14\n",
      "15\n",
      "16\n",
      "17\n",
      "18\n",
      "19\n",
      "'_embedded'\n",
      "There was an issue in the pipeline\n",
      "Here is the last requests response \n",
      "VVVVVVVV\n",
      "{\"errors\":[{\"code\":\"DIS1035\",\"detail\":\"API Limits Exceeded: Max paging depth exceeded. (page * size) must be less than 1,000\",\"status\":\"400\",\"_links\":{\"about\":{\"href\":\"/discovery/v2/errors.html#DIS1035\"}}}]}\n",
      "\n",
      "Rows extracted: 1000\n",
      "Prepare data to have BigQuery necessary formatting.\n",
      "The table events has been cleaned.\n",
      "Loaded 1000 rows for object events into stage:events_tb.\n",
      "\u001b[0m06:04:24  Running with dbt=1.8.8\n",
      "\u001b[0m06:04:24  Registered adapter: bigquery=1.8.3\n",
      "\u001b[0m06:04:24  [\u001b[33mWARNING\u001b[0m]: Configuration paths exist in your dbt_project.yml file which do not apply to any resources.\n",
      "There are 2 unused configuration paths:\n",
      "- models.stage.stage_db\n",
      "- models.stage.production\n",
      "\u001b[0m06:04:24  Found 9 models, 4 data tests, 11 sources, 479 macros\n",
      "\u001b[0m06:04:24  The selection criterion 'dbt' does not match any enabled nodes\n",
      "\u001b[0m06:04:24  The selection criterion 'run' does not match any enabled nodes\n",
      "\u001b[0m06:04:24  The selection criterion '--select' does not match any enabled nodes\n",
      "\u001b[0m06:04:24  \n",
      "\u001b[0m06:04:25  Concurrency: 4 threads (target='dev')\n",
      "\u001b[0m06:04:25  \n",
      "\u001b[0m06:04:25  1 of 6 START sql incremental model production.classification_elt ............... [RUN]\n",
      "\u001b[0m06:04:25  2 of 6 START sql incremental model production.event_attractions_elt ............ [RUN]\n",
      "\u001b[0m06:04:25  3 of 6 START sql incremental model production.events_elt ....................... [RUN]\n",
      "\u001b[0m06:04:25  4 of 6 START sql incremental model production.priceranges_elt .................. [RUN]\n",
      "\u001b[0m06:04:28  3 of 6 OK created sql incremental model production.events_elt .................. [\u001b[32mMERGE (28.0 rows, 219.5 KiB processed)\u001b[0m in 2.92s]\n",
      "\u001b[0m06:04:28  5 of 6 START sql incremental model production.products_elt ..................... [RUN]\n",
      "\u001b[0m06:04:28  4 of 6 OK created sql incremental model production.priceranges_elt ............. [\u001b[32mMERGE (19.0 rows, 41.2 KiB processed)\u001b[0m in 3.03s]\n",
      "\u001b[0m06:04:28  6 of 6 START sql incremental model production.venues_elt ....................... [RUN]\n",
      "\u001b[0m06:04:28  2 of 6 OK created sql incremental model production.event_attractions_elt ....... [\u001b[32mMERGE (33.0 rows, 327.2 KiB processed)\u001b[0m in 3.07s]\n",
      "\u001b[0m06:04:30  1 of 6 OK created sql incremental model production.classification_elt .......... [\u001b[32mMERGE (28.0 rows, 150.0 KiB processed)\u001b[0m in 4.25s]\n",
      "\u001b[0m06:04:31  6 of 6 OK created sql incremental model production.venues_elt .................. [\u001b[32mMERGE (28.0 rows, 418.6 KiB processed)\u001b[0m in 2.99s]\n",
      "\u001b[0m06:04:32  5 of 6 OK created sql incremental model production.products_elt ................ [\u001b[32mMERGE (2.0 rows, 55.5 KiB processed)\u001b[0m in 3.42s]\n",
      "\u001b[0m06:04:32  \n",
      "\u001b[0m06:04:32  Finished running 6 incremental models in 0 hours 0 minutes and 7.91 seconds (7.91s).\n",
      "\u001b[0m06:04:32  \n",
      "\u001b[0m06:04:32  \u001b[32mCompleted successfully\u001b[0m\n",
      "\u001b[0m06:04:32  \n",
      "\u001b[0m06:04:32  Done. PASS=6 WARN=0 ERROR=0 SKIP=0 TOTAL=6\n",
      "events_elt: success\n",
      "priceranges_elt: success\n",
      "event_attractions_elt: success\n",
      "classification_elt: success\n",
      "venues_elt: success\n",
      "products_elt: success\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "\n",
    "# Loop back in every month in the past year\n",
    "\n",
    "date_to_process = '2023-11-01T00:00:00Z'\n",
    "\n",
    "ticketmaster_download_data('events', date_to_process, date_to_process, date_to_process,'50')\n",
    "\n",
    "for object in ['events']:\n",
    "    \n",
    "    # 3 , for deleting the data\n",
    "    query_string = f\"\"\"DROP TABLE `ticketmasterargodemo.stage.{object}_tb`;\"\"\"\n",
    "    results = client.query_and_wait(query_string)\n",
    "\n",
    "    print(f'The table {object} has been cleaned.')\n",
    "\n",
    "\n",
    "upload_data_to_bigquer('events')\n",
    "\n",
    "\n",
    "# execute dbt commands\n",
    "dbt = dbtRunner()\n",
    "cli_args = [\"run\",\"--select\",\"events_elt.sql classification_elt.sql event_attractions_elt.sql priceranges_elt.sql products_elt.sql venues_elt.sql\"]\n",
    "\n",
    "res: dbtRunnerResult = dbt.invoke(cli_args)\n",
    "\n",
    "for r in res.result:\n",
    "    print(f\"{r.node.name}: {r.status}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def send_email_notification(send_to_emails, subject, message):\n",
    "\n",
    "    email = 'korean.markus1992@gmail.com'\n",
    "    password = 'fasdfasdf'\n",
    "\n",
    "    server = smtplib.SMTP('smtp.gmail.com', 587)\n",
    "    server.starttls()\n",
    "    server.login(email, password)\n",
    "\n",
    "    # Loop over each email to send to\n",
    "    for send_to_email in send_to_emails:\n",
    "\n",
    "        # Setup MIMEMultipart for each email address (if we don't do this, the emails will concatenate on each email sent)\n",
    "        msg = MIMEMultipart()\n",
    "        msg['From'] = email\n",
    "        msg['To'] = send_to_email\n",
    "        msg['Subject'] = subject\n",
    "\n",
    "        # Attach the message to the MIMEMultipart object\n",
    "        msg.attach(MIMEText(message, 'plain'))\n",
    "        \n",
    "        # Send the email to this specific email address\n",
    "        server.sendmail(email, send_to_email, msg.as_string())\n",
    "\n",
    "    # Quit the email server when everything is done\n",
    "    server.quit()\n",
    "\n",
    "\n",
    "\n",
    "send_email_notification(['marcosfgalindog@hotmail.com']\n",
    "                             ,'Error in ticketmaster project pipeline '\n",
    "                             ,'Here goes the respose.')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "# queries to test connection\n",
    "# 1\n",
    "\n",
    "# query_string = \"\"\"SELECT name, SUM(number) as total\n",
    "# FROM `bigquery-public-data.usa_names.usa_1910_current`\n",
    "# WHERE name = 'William'\n",
    "# GROUP BY name;\n",
    "# \"\"\"\n",
    "# results = client.query_and_wait(query_string)\n",
    "\n",
    "# # Print the results.\n",
    "# for row in results:  # Wait for the job to complete.\n",
    "#     print(\"{}: {}\".format(row[\"name\"], row[\"total\"]))\n",
    "\n",
    "\n",
    "\n",
    "# 2\n",
    "# query_string = \"\"\"SELECT *\n",
    "# FROM `bigquer\n",
    "# y-public-data.usa_names.usa_1910_current`\n",
    "# WHERE name = 'William'\n",
    "# ;\n",
    "# \"\"\"\n",
    "# results = client.query_and_wait(query_string)\n",
    "\n",
    "# for row in results:\n",
    "#     print(row)\n",
    "\n",
    "\n",
    "# 3 , for deleting the data\n",
    "# query_string = \"\"\"TRUNCATE TABLE `ticketmaster-demo-argo.stage_db.events_tb`;\n",
    "# \"\"\"\n",
    "# results = client.query_and_wait(query_string)\n",
    "\n",
    "# for i in results:\n",
    "#     print(i)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ticketmaster_poc",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
